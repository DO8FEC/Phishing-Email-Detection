{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d80f7b839b27ab1",
   "metadata": {},
   "source": [
    "# üéì Understanding XGBoost for Phishing Email Classification üìßüå≥üöÄ\n",
    "\n",
    "### What is XGBoost? ü§î\n",
    "**XGBoost** (eXtreme Gradient Boosting) is a **supervised learning algorithm** based on **boosting**. It is designed for both **classification** and **regression** tasks and is known for its speed and performance. XGBoost creates an ensemble of **decision trees**, where each new tree is trained **sequentially**, correcting the errors made by the previous trees. In this way, each tree **learns from the mistakes of the previous one**, gradually improving the model‚Äôs accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### How XGBoost Works üõ†Ô∏è\n",
    "\n",
    "1. **Boosting Technique**:\n",
    "   - XGBoost uses a method called **boosting**, where trees are added **sequentially**, and each new tree tries to correct the errors of the previous ones.\n",
    "   - Unlike **Random Forest**, which trains all trees in parallel, XGBoost trains trees sequentially, allowing each tree to learn from the mistakes of the previous trees, thereby improving the overall model.\n",
    "\n",
    "2. **Gradient Descent Optimization**:\n",
    "   - XGBoost uses **gradient descent** to minimize the error made by the ensemble of trees. It adjusts the model by minimizing a **loss function** through **gradient updates**.\n",
    "   \n",
    "3. **Tree Pruning**:\n",
    "   - XGBoost prunes trees during training by using a **regularization term**, which prevents overfitting. This ensures that the trees are not overly complex and helps with generalization.\n",
    "\n",
    "4. **Weighted Trees**:\n",
    "   - Each tree is weighted based on its performance. Poorly performing trees are given more focus, and new trees aim to correct their errors, improving overall performance.\n",
    "\n",
    "5. **Learning Rate**:\n",
    "   - The model uses a **learning rate** to control the contribution of each tree. A lower learning rate makes the model slower to learn but allows for a more accurate model with more trees.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of XGBoost for Phishing Email Classification üìß‚ú®\n",
    "\n",
    "- **High Accuracy**: XGBoost is known for its exceptional accuracy due to its boosting technique and ability to handle complex patterns in the data.\n",
    "- **Regularization**: It includes **L1 and L2 regularization** to prevent overfitting, which makes it more robust, even with noisy data.\n",
    "- **Handles Missing Data**: XGBoost can automatically handle missing data by learning the best direction to split on for missing values.\n",
    "\n",
    "---\n",
    "\n",
    "### Potential Limitations:\n",
    "- **Complexity**: While XGBoost provides high accuracy, it can be more complex to understand and tune compared to simpler models like logistic regression or decision trees.\n",
    "- **Sensitive to Hyperparameters**: XGBoost has many hyperparameters to tune (learning rate, tree depth, regularization, etc.), and improper tuning can affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5651d4e-95c3-49c8-bf60-fba4e8fe166f",
   "metadata": {},
   "source": [
    "### Implementation üîç\n",
    "1. **Loading the required libraries** üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdd0f24-0e85-4ca0-a3eb-8234602f5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44d375-dbe9-4da5-8b37-20f11ff84ef0",
   "metadata": {},
   "source": [
    "2. **Loading and splitting the Data** üì•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8989cec7-4ca8-4a48-9e89-4029d76ebada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the save TF-IDF features and labels\n",
    "x_data = np.load('../feature_x.npy')\n",
    "y_data = np.load('../y_tf.npy')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067b0c3-48e4-407d-ab12-066c432f22a7",
   "metadata": {},
   "source": [
    "3. **Model Initialization** ü§ñ\n",
    "\n",
    "The **`XGBClassifier()`** is initialized with its **default parameters** in the **XGBoost** library. This classifier uses **gradient boosting** to combine the predictions of multiple decision trees, improving the overall model's performance for classification tasks.\n",
    "\n",
    "- **`objective=\"binary:logistic\"`**: The default objective function is **logistic regression** for binary classification tasks. It calculates the probability that a given sample belongs to one of the two classes.\n",
    "  \n",
    "- **`learning_rate=0.1`**: This parameter controls the **step size** during the boosting process. A lower learning rate makes the model learn more slowly but more accurately, while a higher rate makes it learn faster but could lead to overfitting.\n",
    "\n",
    "- **`n_estimators=100`**: The number of **trees** the model will build. Each tree attempts to correct the errors of the previous trees, making the overall model stronger.\n",
    "\n",
    "- **`max_depth=3`**: The maximum depth of each individual decision tree. Shallower trees prevent overfitting but may underfit, while deeper trees might overfit the training data.\n",
    "\n",
    "- **`subsample=1.0`**: This parameter controls the percentage of the training data that is used to grow each tree. A value less than 1.0 can help prevent overfitting by introducing randomness into the training process.\n",
    "\n",
    "- **`colsample_bytree=1.0`**: The fraction of features (columns) to be used by each tree. Reducing this can help with generalization by training each tree with different subsets of features.\n",
    "\n",
    "- **`gamma=0`**: This is the **regularization parameter** that controls how much the model attempts to split nodes. Higher values make the algorithm more conservative and prevent overfitting.\n",
    "\n",
    "- **`random_state=None`**: This controls the randomness for reproducibility. By default, the model does not use a fixed random seed, but setting this parameter will ensure consistent results across different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bc648b-0540-455b-a179-9bd72c083bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab4acc-2771-40d9-8d19-c0f2ab795f50",
   "metadata": {},
   "source": [
    "4. **Training the Model** üèãÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d2e7f-2bfc-4dfc-a743-6635270a1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4498e68-e346-4c87-a3cd-29067b411309",
   "metadata": {},
   "source": [
    "5. **Making Predictions** üîÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1c94-1a1f-4310-9d5e-74fd8e70f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8e5b7-6af7-48b7-a5e8-e533eff20e71",
   "metadata": {},
   "source": [
    "6. **Evaluating the Model** üßÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fa34f-698b-484a-9e9b-3fed994ba034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy from XGB:{accuracy_score(y_test,prediction)*100:.5f} %\")\n",
    "print(f\"f1 score from XGB: {f1_score(y_test,prediction)*100:.5f} %\")\n",
    "print(\"classification report : \\n\",classification_report(y_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
